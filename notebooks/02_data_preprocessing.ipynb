{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rock-Paper-Scissors CNN Project\n",
        "## 2. Data Preprocessing and Augmentation\n",
        "\n",
        "This notebook handles data preprocessing, normalization, augmentation, and train/validation/test splitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import yaml\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "from data.data_loader import RockPaperScissorsDataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration and Setup\n",
        "\n",
        "Let's load the configuration and set up the data preprocessing pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = '../config/config.yaml'\n",
        "with open(config_path, 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Extract configuration parameters\n",
        "data_config = config['data']\n",
        "augmentation_config = config['augmentation']\n",
        "classes = config['classes']\n",
        "\n",
        "print(\"CONFIGURATION LOADED\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Image size: {data_config['image_size']}\")\n",
        "print(f\"Batch size: {data_config['batch_size']}\")\n",
        "print(f\"Validation split: {data_config['validation_split']}\")\n",
        "print(f\"Test split: {data_config['test_split']}\")\n",
        "print(f\"Random seed: {data_config['random_seed']}\")\n",
        "print(f\"Classes: {classes}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize data loader\n",
        "loader = RockPaperScissorsDataLoader(config_path)\n",
        "print(\"✅ Data loader initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading and Information\n",
        "\n",
        "First, let's load the dataset and examine its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset information\n",
        "dataset_info = loader.load_dataset_info()\n",
        "\n",
        "# Display dataset summary\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "total_images = 0\n",
        "for class_name, info in dataset_info.items():\n",
        "    if class_name != 'total':\n",
        "        count = info['count']\n",
        "        total_images += count\n",
        "        print(f\"{class_name.upper()}: {count:,} images\")\n",
        "\n",
        "print(f\"\\nTOTAL: {total_images:,} images\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check if we have data\n",
        "if total_images == 0:\n",
        "    print(\"❌ No data found! Please ensure the dataset is in the correct location.\")\n",
        "    print(\"Expected structure:\")\n",
        "    print(\"data/raw/\")\n",
        "    print(\"├── rock/\")\n",
        "    print(\"├── paper/\")\n",
        "    print(\"└── scissors/\")\n",
        "else:\n",
        "    print(\"✅ Dataset loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "\n",
        "Now let's split the dataset into training, validation, and test sets following proper ML practices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset into train/validation/test sets\n",
        "if total_images > 0:\n",
        "    print(\"SPLITTING DATASET...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Perform the split\n",
        "    split_info, split_dirs = loader.split_dataset(dataset_info)\n",
        "    \n",
        "    # Display split results\n",
        "    print(\"\\nSPLIT RESULTS:\")\n",
        "    print(\"-\" * 30)\n",
        "    total_train = 0\n",
        "    total_val = 0\n",
        "    total_test = 0\n",
        "    \n",
        "    for class_name, splits in split_info.items():\n",
        "        train_count = len(splits['train'])\n",
        "        val_count = len(splits['val'])\n",
        "        test_count = len(splits['test'])\n",
        "        \n",
        "        total_train += train_count\n",
        "        total_val += val_count\n",
        "        total_test += test_count\n",
        "        \n",
        "        print(f\"{class_name.upper()}:\")\n",
        "        print(f\"  Train: {train_count:,} images\")\n",
        "        print(f\"  Val:   {val_count:,} images\")\n",
        "        print(f\"  Test:  {test_count:,} images\")\n",
        "        print()\n",
        "    \n",
        "    print(\"TOTAL SPLIT:\")\n",
        "    print(f\"  Train: {total_train:,} images ({total_train/(total_train+total_val+total_test)*100:.1f}%)\")\n",
        "    print(f\"  Val:   {total_val:,} images ({total_val/(total_train+total_val+total_test)*100:.1f}%)\")\n",
        "    print(f\"  Test:  {total_test:,} images ({total_test/(total_train+total_val+total_test)*100:.1f}%)\")\n",
        "    \n",
        "    # Save split information\n",
        "    loader.save_split_info(split_info)\n",
        "    print(\"\\n✅ Dataset split completed and saved!\")\n",
        "    \n",
        "    # Visualize the split\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Bar plot of split\n",
        "    split_counts = [total_train, total_val, total_test]\n",
        "    split_labels = ['Train', 'Validation', 'Test']\n",
        "    colors = ['#2E8B57', '#4169E1', '#DC143C']\n",
        "    \n",
        "    bars = ax1.bar(split_labels, split_counts, color=colors)\n",
        "    ax1.set_title('Dataset Split Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Number of Images')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, split_counts):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
        "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Pie chart of split\n",
        "    ax2.pie(split_counts, labels=split_labels, autopct='%1.1f%%', \n",
        "            colors=colors, startangle=90)\n",
        "    ax2.set_title('Dataset Split Percentages', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Cannot split dataset - no data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Let's set up data augmentation to improve model generalization and prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up data augmentation\n",
        "print(\"DATA AUGMENTATION CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Training data generator with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=augmentation_config['rotation_range'],\n",
        "    width_shift_range=augmentation_config['width_shift_range'],\n",
        "    height_shift_range=augmentation_config['height_shift_range'],\n",
        "    horizontal_flip=augmentation_config['horizontal_flip'],\n",
        "    zoom_range=augmentation_config['zoom_range'],\n",
        "    fill_mode=augmentation_config['fill_mode'],\n",
        "    rescale=1./255  # Normalize pixel values to [0,1]\n",
        ")\n",
        "\n",
        "# Validation and test data generators (no augmentation, only normalization)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Training Data Augmentation:\")\n",
        "print(f\"  Rotation range: ±{augmentation_config['rotation_range']}°\")\n",
        "print(f\"  Width shift: ±{augmentation_config['width_shift_range']*100:.1f}%\")\n",
        "print(f\"  Height shift: ±{augmentation_config['height_shift_range']*100:.1f}%\")\n",
        "print(f\"  Horizontal flip: {augmentation_config['horizontal_flip']}\")\n",
        "print(f\"  Zoom range: ±{augmentation_config['zoom_range']*100:.1f}%\")\n",
        "print(f\"  Fill mode: {augmentation_config['fill_mode']}\")\n",
        "print(f\"  Rescaling: 1/255 (normalization)\")\n",
        "print(\"\\nValidation/Test Data:\")\n",
        "print(f\"  Rescaling: 1/255 (normalization only)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Visualize augmentation effects\n",
        "if total_images > 0:\n",
        "    print(\"\\nVISUALIZING DATA AUGMENTATION...\")\n",
        "    \n",
        "    # Get a sample image\n",
        "    sample_class = classes[0]\n",
        "    sample_path = Path(f\"../data/processed/train/{sample_class}\")\n",
        "    if sample_path.exists():\n",
        "        sample_images = list(sample_path.glob('*.png'))\n",
        "        if sample_images:\n",
        "            # Load original image\n",
        "            original_img = Image.open(sample_images[0])\n",
        "            original_array = np.array(original_img) / 255.0\n",
        "            \n",
        "            # Create augmented versions\n",
        "            fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "            fig.suptitle(f'Data Augmentation Examples - {sample_class.capitalize()}', \n",
        "                        fontsize=16, fontweight='bold')\n",
        "            \n",
        "            # Original image\n",
        "            axes[0, 0].imshow(original_array)\n",
        "            axes[0, 0].set_title('Original', fontweight='bold')\n",
        "            axes[0, 0].axis('off')\n",
        "            \n",
        "            # Generate augmented images\n",
        "            augmented_images = []\n",
        "            for i in range(7):\n",
        "                # Reshape for ImageDataGenerator\n",
        "                img_reshaped = original_array.reshape((1,) + original_array.shape)\n",
        "                \n",
        "                # Generate augmented image\n",
        "                aug_iter = train_datagen.flow(img_reshaped, batch_size=1)\n",
        "                aug_img = next(aug_iter)[0]\n",
        "                augmented_images.append(aug_img)\n",
        "            \n",
        "            # Display augmented images\n",
        "            titles = ['Rotation', 'Width Shift', 'Height Shift', 'Horizontal Flip']\n",
        "            for i, (img, title) in enumerate(zip(augmented_images[:4], titles)):\n",
        "                axes[0, i+1].imshow(img)\n",
        "                axes[0, i+1].set_title(title, fontweight='bold')\n",
        "                axes[0, i+1].axis('off')\n",
        "            \n",
        "            # More augmentations\n",
        "            titles2 = ['Zoom', 'Combined 1', 'Combined 2']\n",
        "            for i, (img, title) in enumerate(zip(augmented_images[4:7], titles2)):\n",
        "                axes[1, i].imshow(img)\n",
        "                axes[1, i].set_title(title, fontweight='bold')\n",
        "                axes[1, i].axis('off')\n",
        "            \n",
        "            # Hide the last subplot\n",
        "            axes[1, 3].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(\"✅ Data augmentation visualization completed!\")\n",
        "        else:\n",
        "            print(\"⚠️ No sample images found for visualization\")\n",
        "    else:\n",
        "        print(\"⚠️ Sample directory not found for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Generators Setup\n",
        "\n",
        "Now let's create the data generators for training, validation, and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data generators\n",
        "if total_images > 0:\n",
        "    print(\"CREATING DATA GENERATORS...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Create generators using the data loader\n",
        "    train_generator, val_generator, test_generator = loader.create_data_generators(\n",
        "        str(split_dirs['train']),\n",
        "        str(split_dirs['val']),\n",
        "        str(split_dirs['test'])\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Data generators created successfully!\")\n",
        "    print(f\"Training batches: {len(train_generator)}\")\n",
        "    print(f\"Validation batches: {len(val_generator)}\")\n",
        "    print(f\"Test batches: {len(test_generator)}\")\n",
        "    \n",
        "    # Display generator information\n",
        "    print(\"\\nGENERATOR INFORMATION:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Batch size: {data_config['batch_size']}\")\n",
        "    print(f\"Image size: {data_config['image_size']}\")\n",
        "    print(f\"Number of classes: {len(classes)}\")\n",
        "    print(f\"Class indices: {train_generator.class_indices}\")\n",
        "    \n",
        "    # Test the generators\n",
        "    print(\"\\nTESTING GENERATORS...\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Get a batch from training generator\n",
        "    train_batch_x, train_batch_y = next(train_generator)\n",
        "    print(f\"Training batch shape: {train_batch_x.shape}\")\n",
        "    print(f\"Training labels shape: {train_batch_y.shape}\")\n",
        "    print(f\"Training batch data type: {train_batch_x.dtype}\")\n",
        "    print(f\"Training batch value range: [{train_batch_x.min():.3f}, {train_batch_x.max():.3f}]\")\n",
        "    \n",
        "    # Get a batch from validation generator\n",
        "    val_batch_x, val_batch_y = next(val_generator)\n",
        "    print(f\"Validation batch shape: {val_batch_x.shape}\")\n",
        "    print(f\"Validation labels shape: {val_batch_y.shape}\")\n",
        "    \n",
        "    # Visualize a batch of training images\n",
        "    print(\"\\nVISUALIZING TRAINING BATCH...\")\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    fig.suptitle('Sample Training Batch (with Augmentation)', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i in range(8):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        # Display image\n",
        "        axes[row, col].imshow(train_batch_x[i])\n",
        "        \n",
        "        # Get class label\n",
        "        class_idx = np.argmax(train_batch_y[i])\n",
        "        class_name = list(train_generator.class_indices.keys())[class_idx]\n",
        "        \n",
        "        axes[row, col].set_title(f'{class_name.capitalize()}', fontweight='bold')\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"✅ Data generators are working correctly!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Cannot create data generators - no data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary and Next Steps\n",
        "\n",
        "Let's summarize what we've accomplished in the data preprocessing phase and prepare for model development.\n",
        "\n",
        "**Data Preprocessing Summary:**\n",
        "1. **Dataset Loading**: Successfully loaded and analyzed the Rock-Paper-Scissors dataset\n",
        "2. **Data Splitting**: Properly split data into train/validation/test sets (70/20/10)\n",
        "3. **Data Augmentation**: Implemented comprehensive augmentation strategies\n",
        "4. **Data Generators**: Created efficient data generators for training\n",
        "\n",
        "**Key Preprocessing Steps Completed:**\n",
        "✅ **Image Resizing**: Images resized to 224x224 pixels\n",
        "✅ **Normalization**: Pixel values normalized to [0,1] range\n",
        "✅ **Data Augmentation**: Rotation, shift, zoom, and flip augmentations\n",
        "✅ **Train/Val/Test Split**: Proper splitting with no data leakage\n",
        "✅ **Data Generators**: Efficient batch loading with augmentation\n",
        "\n",
        "**Project Requirements Addressed:**\n",
        "✅ **Data Preprocessing**: Image resizing and normalization implemented\n",
        "✅ **Data Augmentation**: Comprehensive augmentation techniques applied\n",
        "✅ **Data Splitting**: Proper train/validation/test split (no test set information used)\n",
        "✅ **Sound Methodology**: Following ML best practices for data handling\n",
        "\n",
        "**Next Steps:**\n",
        "- Model architecture design (Simple, Medium, Complex CNNs)\n",
        "- Model training with proper callbacks\n",
        "- Hyperparameter tuning\n",
        "- Model evaluation and analysis\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
