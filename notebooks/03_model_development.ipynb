{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rock-Paper-Scissors CNN Project\n",
        "## 3. Model Development and Training\n",
        "\n",
        "This notebook focuses on designing and training different CNN architectures with increasing complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "from models.cnn_models import RockPaperScissorsCNN\n",
        "from utils.training_utils import TrainingManager\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CPU cores: {tf.config.threading.get_inter_op_parallelism_threads()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration and Setup\n",
        "\n",
        "Let's load the configuration and set up the model development environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = '../config/config.yaml'\n",
        "with open(config_path, 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Extract configuration parameters\n",
        "model_configs = config['models']\n",
        "training_config = config['training']\n",
        "data_config = config['data']\n",
        "classes = config['classes']\n",
        "\n",
        "print(\"CONFIGURATION LOADED\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Number of classes: {len(classes)}\")\n",
        "print(f\"Classes: {classes}\")\n",
        "print(f\"Image size: {data_config['image_size']}\")\n",
        "print(f\"Batch size: {data_config['batch_size']}\")\n",
        "print(f\"Training epochs: {training_config['epochs']}\")\n",
        "print(f\"Learning rate: {training_config['learning_rate']}\")\n",
        "print(f\"Optimizer: {training_config['optimizer']}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize model creator and training manager\n",
        "cnn_creator = RockPaperScissorsCNN(config_path)\n",
        "trainer = TrainingManager(config_path)\n",
        "\n",
        "print(\"✅ Model creator and training manager initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Generators Setup\n",
        "\n",
        "Let's set up the data generators for training and validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up data generators (assuming data preprocessing was completed)\n",
        "print(\"SETTING UP DATA GENERATORS...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check if processed data exists\n",
        "train_dir = '../data/processed/train'\n",
        "val_dir = '../data/processed/val'\n",
        "test_dir = '../data/processed/test'\n",
        "\n",
        "if os.path.exists(train_dir) and os.path.exists(val_dir):\n",
        "    # Create data generators\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "        rescale=1./255\n",
        "    )\n",
        "    \n",
        "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # Create generators\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=tuple(data_config['image_size']),\n",
        "        batch_size=data_config['batch_size'],\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "    \n",
        "    val_generator = val_test_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=tuple(data_config['image_size']),\n",
        "        batch_size=data_config['batch_size'],\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    test_generator = None\n",
        "    if os.path.exists(test_dir):\n",
        "        test_generator = val_test_datagen.flow_from_directory(\n",
        "            test_dir,\n",
        "            target_size=tuple(data_config['image_size']),\n",
        "            batch_size=data_config['batch_size'],\n",
        "            class_mode='categorical',\n",
        "            shuffle=False\n",
        "        )\n",
        "    \n",
        "    print(\"✅ Data generators created successfully!\")\n",
        "    print(f\"Training samples: {train_generator.samples}\")\n",
        "    print(f\"Validation samples: {val_generator.samples}\")\n",
        "    if test_generator:\n",
        "        print(f\"Test samples: {test_generator.samples}\")\n",
        "    print(f\"Class indices: {train_generator.class_indices}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Processed data not found!\")\n",
        "    print(\"Please run the data preprocessing notebook first.\")\n",
        "    print(\"Expected directories:\")\n",
        "    print(f\"  {train_dir}\")\n",
        "    print(f\"  {val_dir}\")\n",
        "    print(f\"  {test_dir}\")\n",
        "    \n",
        "    # Create dummy generators for demonstration\n",
        "    print(\"\\n⚠️ Creating dummy generators for demonstration...\")\n",
        "    \n",
        "    # Create dummy data\n",
        "    dummy_x = np.random.random((32, 224, 224, 3))\n",
        "    dummy_y = np.random.random((32, 3))\n",
        "    \n",
        "    class DummyGenerator:\n",
        "        def __init__(self, x, y):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.samples = len(x)\n",
        "            self.class_indices = {'paper': 0, 'rock': 1, 'scissors': 2}\n",
        "        \n",
        "        def __iter__(self):\n",
        "            return self\n",
        "        \n",
        "        def __next__(self):\n",
        "            return self.x, self.y\n",
        "    \n",
        "    train_generator = DummyGenerator(dummy_x, dummy_y)\n",
        "    val_generator = DummyGenerator(dummy_x, dummy_y)\n",
        "    test_generator = DummyGenerator(dummy_x, dummy_y)\n",
        "    \n",
        "    print(\"✅ Dummy generators created for demonstration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Architecture Design\n",
        "\n",
        "Now let's design and create the three CNN architectures with increasing complexity as required by the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the three CNN architectures\n",
        "print(\"CREATING CNN ARCHITECTURES...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# 1. Simple CNN\n",
        "print(\"\\n1. SIMPLE CNN ARCHITECTURE\")\n",
        "print(\"-\" * 40)\n",
        "simple_model = cnn_creator.create_simple_cnn(input_shape)\n",
        "print(\"Model Summary:\")\n",
        "print(cnn_creator.get_model_summary(simple_model))\n",
        "\n",
        "# 2. Medium CNN\n",
        "print(\"\\n2. MEDIUM CNN ARCHITECTURE\")\n",
        "print(\"-\" * 40)\n",
        "medium_model = cnn_creator.create_medium_cnn(input_shape)\n",
        "print(\"Model Summary:\")\n",
        "print(cnn_creator.get_model_summary(medium_model))\n",
        "\n",
        "# 3. Complex CNN\n",
        "print(\"\\n3. COMPLEX CNN ARCHITECTURE\")\n",
        "print(\"-\" * 40)\n",
        "complex_model = cnn_creator.create_complex_cnn(input_shape)\n",
        "print(\"Model Summary:\")\n",
        "print(cnn_creator.get_model_summary(complex_model))\n",
        "\n",
        "# Compare model complexities\n",
        "print(\"\\nMODEL COMPLEXITY COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "models_info = [\n",
        "    (\"Simple CNN\", simple_model),\n",
        "    (\"Medium CNN\", medium_model),\n",
        "    (\"Complex CNN\", complex_model)\n",
        "]\n",
        "\n",
        "for name, model in models_info:\n",
        "    params = model.count_params()\n",
        "    print(f\"{name:15}: {params:,} parameters\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training\n",
        "\n",
        "Now let's train each model and compare their performance. We'll start with the Simple CNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Simple CNN\n",
        "print(\"TRAINING SIMPLE CNN...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train the model\n",
        "simple_history = trainer.train_model(\n",
        "    simple_model, \n",
        "    train_generator, \n",
        "    val_generator, \n",
        "    \"simple_cnn\"\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "trainer.plot_training_history(simple_history, \"simple_cnn\")\n",
        "\n",
        "# Save the model\n",
        "cnn_creator.save_model(simple_model, \"simple_cnn\")\n",
        "\n",
        "print(\"✅ Simple CNN training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Medium CNN\n",
        "print(\"TRAINING MEDIUM CNN...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train the model\n",
        "medium_history = trainer.train_model(\n",
        "    medium_model, \n",
        "    train_generator, \n",
        "    val_generator, \n",
        "    \"medium_cnn\"\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "trainer.plot_training_history(medium_history, \"medium_cnn\")\n",
        "\n",
        "# Save the model\n",
        "cnn_creator.save_model(medium_model, \"medium_cnn\")\n",
        "\n",
        "print(\"✅ Medium CNN training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Complex CNN\n",
        "print(\"TRAINING COMPLEX CNN...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train the model\n",
        "complex_history = trainer.train_model(\n",
        "    complex_model, \n",
        "    train_generator, \n",
        "    val_generator, \n",
        "    \"complex_cnn\"\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "trainer.plot_training_history(complex_history, \"complex_cnn\")\n",
        "\n",
        "# Save the model\n",
        "cnn_creator.save_model(complex_model, \"complex_cnn\")\n",
        "\n",
        "print(\"✅ Complex CNN training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Results Comparison\n",
        "\n",
        "Let's compare the training results of all three models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare training results\n",
        "print(\"TRAINING RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Collect training histories\n",
        "histories = {\n",
        "    \"Simple CNN\": simple_history,\n",
        "    \"Medium CNN\": medium_history,\n",
        "    \"Complex CNN\": complex_history\n",
        "}\n",
        "\n",
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Model Training Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot training accuracy\n",
        "for name, history in histories.items():\n",
        "    axes[0, 0].plot(history.history['accuracy'], label=f'{name} (Train)', linestyle='-')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label=f'{name} (Val)', linestyle='--')\n",
        "\n",
        "axes[0, 0].set_title('Training and Validation Accuracy')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot training loss\n",
        "for name, history in histories.items():\n",
        "    axes[0, 1].plot(history.history['loss'], label=f'{name} (Train)', linestyle='-')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label=f'{name} (Val)', linestyle='--')\n",
        "\n",
        "axes[0, 1].set_title('Training and Validation Loss')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot final validation accuracy comparison\n",
        "model_names = list(histories.keys())\n",
        "final_val_acc = [max(history.history['val_accuracy']) for history in histories.values()]\n",
        "final_train_acc = [max(history.history['accuracy']) for history in histories.values()]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = axes[1, 0].bar(x - width/2, final_train_acc, width, label='Training', alpha=0.8)\n",
        "bars2 = axes[1, 0].bar(x + width/2, final_val_acc, width, label='Validation', alpha=0.8)\n",
        "\n",
        "axes[1, 0].set_title('Final Accuracy Comparison')\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot final validation loss comparison\n",
        "final_val_loss = [min(history.history['val_loss']) for history in histories.values()]\n",
        "final_train_loss = [min(history.history['loss']) for history in histories.values()]\n",
        "\n",
        "bars3 = axes[1, 1].bar(x - width/2, final_train_loss, width, label='Training', alpha=0.8)\n",
        "bars4 = axes[1, 1].bar(x + width/2, final_val_loss, width, label='Validation', alpha=0.8)\n",
        "\n",
        "axes[1, 1].set_title('Final Loss Comparison')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(model_names, rotation=45)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars3, bars4]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nTRAINING SUMMARY STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<15} {'Final Train Acc':<15} {'Final Val Acc':<15} {'Final Train Loss':<15} {'Final Val Loss':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name, history in histories.items():\n",
        "    train_acc = max(history.history['accuracy'])\n",
        "    val_acc = max(history.history['val_accuracy'])\n",
        "    train_loss = min(history.history['loss'])\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    \n",
        "    print(f\"{name:<15} {train_acc:<15.4f} {val_acc:<15.4f} {train_loss:<15.4f} {val_loss:<15.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overfitting and Underfitting Analysis\n",
        "\n",
        "Let's analyze the training curves to identify overfitting and underfitting patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze overfitting and underfitting\n",
        "print(\"OVERFITTING AND UNDERFITTING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def analyze_fitting_pattern(history, model_name):\n",
        "    \"\"\"Analyze fitting patterns from training history.\"\"\"\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    \n",
        "    # Calculate gaps\n",
        "    acc_gap = max(train_acc) - max(val_acc)\n",
        "    loss_gap = min(val_loss) - min(train_loss)\n",
        "    \n",
        "    # Determine fitting pattern\n",
        "    if acc_gap > 0.1 or loss_gap > 0.1:\n",
        "        pattern = \"OVERFITTING\"\n",
        "        severity = \"High\" if acc_gap > 0.2 or loss_gap > 0.2 else \"Moderate\"\n",
        "    elif acc_gap < 0.02 and loss_gap < 0.02:\n",
        "        pattern = \"GOOD FIT\"\n",
        "        severity = \"Good\"\n",
        "    else:\n",
        "        pattern = \"SLIGHT OVERFITTING\"\n",
        "        severity = \"Low\"\n",
        "    \n",
        "    # Check for underfitting\n",
        "    if max(val_acc) < 0.7 and max(train_acc) < 0.8:\n",
        "        pattern = \"UNDERFITTING\"\n",
        "        severity = \"High\"\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'pattern': pattern,\n",
        "        'severity': severity,\n",
        "        'acc_gap': acc_gap,\n",
        "        'loss_gap': loss_gap,\n",
        "        'final_train_acc': max(train_acc),\n",
        "        'final_val_acc': max(val_acc),\n",
        "        'final_train_loss': min(train_loss),\n",
        "        'final_val_loss': min(val_loss)\n",
        "    }\n",
        "\n",
        "# Analyze each model\n",
        "analyses = []\n",
        "for name, history in histories.items():\n",
        "    analysis = analyze_fitting_pattern(history, name)\n",
        "    analyses.append(analysis)\n",
        "\n",
        "# Display analysis results\n",
        "print(f\"{'Model':<15} {'Pattern':<20} {'Severity':<10} {'Acc Gap':<10} {'Loss Gap':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for analysis in analyses:\n",
        "    print(f\"{analysis['model']:<15} {analysis['pattern']:<20} {analysis['severity']:<10} \"\n",
        "          f\"{analysis['acc_gap']:<10.4f} {analysis['loss_gap']:<10.4f}\")\n",
        "\n",
        "print(\"\\nDETAILED ANALYSIS:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for analysis in analyses:\n",
        "    print(f\"\\n{analysis['model']}:\")\n",
        "    print(f\"  Pattern: {analysis['pattern']}\")\n",
        "    print(f\"  Severity: {analysis['severity']}\")\n",
        "    print(f\"  Final Training Accuracy: {analysis['final_train_acc']:.4f}\")\n",
        "    print(f\"  Final Validation Accuracy: {analysis['final_val_acc']:.4f}\")\n",
        "    print(f\"  Accuracy Gap: {analysis['acc_gap']:.4f}\")\n",
        "    print(f\"  Final Training Loss: {analysis['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Validation Loss: {analysis['final_val_loss']:.4f}\")\n",
        "    print(f\"  Loss Gap: {analysis['loss_gap']:.4f}\")\n",
        "\n",
        "# Recommendations\n",
        "print(\"\\nRECOMMENDATIONS:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for analysis in analyses:\n",
        "    print(f\"\\n{analysis['model']}:\")\n",
        "    if analysis['pattern'] == \"OVERFITTING\":\n",
        "        print(\"  - Consider adding more dropout\")\n",
        "        print(\"  - Reduce model complexity\")\n",
        "        print(\"  - Increase data augmentation\")\n",
        "        print(\"  - Use early stopping\")\n",
        "    elif analysis['pattern'] == \"UNDERFITTING\":\n",
        "        print(\"  - Increase model complexity\")\n",
        "        print(\"  - Train for more epochs\")\n",
        "        print(\"  - Reduce regularization\")\n",
        "        print(\"  - Check learning rate\")\n",
        "    elif analysis['pattern'] == \"GOOD FIT\":\n",
        "        print(\"  - Model is well-balanced\")\n",
        "        print(\"  - Consider fine-tuning hyperparameters\")\n",
        "    else:\n",
        "        print(\"  - Monitor training closely\")\n",
        "        print(\"  - Consider slight adjustments\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary and Next Steps\n",
        "\n",
        "Let's summarize the model development phase and prepare for hyperparameter tuning.\n",
        "\n",
        "**Model Development Summary:**\n",
        "1. **Architecture Design**: Created 3 CNN architectures with increasing complexity\n",
        "2. **Model Training**: Trained all models with proper callbacks and monitoring\n",
        "3. **Performance Comparison**: Analyzed training curves and model performance\n",
        "4. **Overfitting Analysis**: Identified fitting patterns and provided recommendations\n",
        "\n",
        "**Key Achievements:**\n",
        "✅ **Simple CNN**: Basic architecture with 2 conv layers\n",
        "✅ **Medium CNN**: Intermediate architecture with 3 conv layers + batch normalization\n",
        "✅ **Complex CNN**: Advanced architecture with 4 conv layers + global pooling\n",
        "✅ **Training Curves**: Comprehensive visualization of training progress\n",
        "✅ **Overfitting Analysis**: Detailed analysis of fitting patterns\n",
        "\n",
        "**Project Requirements Addressed:**\n",
        "✅ **3 CNN Architectures**: Simple, Medium, and Complex models implemented\n",
        "✅ **Incremental Complexity**: Each model is more complex than the previous\n",
        "✅ **Architecture Justification**: Clear definition of layers, activations, pooling, dropout\n",
        "✅ **Training with Optimizer**: Proper optimizer and loss function usage\n",
        "✅ **Training Curves**: Visualization of loss and accuracy curves\n",
        "✅ **Overfitting Analysis**: Discussion of overfitting and underfitting patterns\n",
        "\n",
        "**Next Steps:**\n",
        "- Hyperparameter tuning for the best performing model\n",
        "- Comprehensive model evaluation on test set\n",
        "- Misclassification analysis\n",
        "- Final model selection and optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
